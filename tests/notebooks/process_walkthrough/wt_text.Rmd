---
title: "Entity Extraction Performance Evaluation - R & Python"
author: "Evan Canfield"
date: "12/30/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Purpose
The purpose of this notebook is to test how to implement the Entity Extraction model 

# Import
## Libraries
```{r import_libraries}
  if (!require(pacman)) {install.packages('pacman')}
  p_load(
    dplyr
  )
```

## Source Files
The following imports functions defined in the sourced R scripts.
```{r}
# # Import All Scripts
# script_path <- "../R/"
# file_paths <- list.files(recursive = TRUE,
#                          path = script_path, pattern = ".R",
#                          full.names = TRUE)
#
# for (file in file_paths){
#   source(file)
# }
```

## Data
```{r}
folder_path <- "./../../../inst/extdata/sample_documents/"
pdf_paths <- list.files(recursive = FALSE, 
                       path = folder_path, 
                       pattern = ".pdf", 
                       full.names = TRUE)
print(pdf_paths)
```

## Assisting Functions
```{r}
#' Inspect Text String
inspect <- function(x, m = 100, span = 20) {
  n = m + span - 1
  x[m:n]
  
}
```

## Regex
```{r}
## Identify Letters
regex_letters <- '[a-zA-Z]'

## Identify IP Address
regex_ip <- "(?:[\\d]{1,3})\\.(?:[\\d]{1,3})\\.(?:[\\d]{1,3})\\.(?:[\\d]{1,3})"

## Identify Parenthesis
regex_parens <- "\\(([^()]+)\\)"

## Identify Numbers
regex_return_num <- "(\\d)+"
```

# Process Data
```{r}
input_path <- pdf_paths[6]
input_path
input_text <- pdfminer$extract_text(input_path)
```

## Step 1
```{r}
processing_text <- input_text %>%
  stringr::str_split(pattern = "\r\n") %>% # Tabulizer
  stringr::str_split(pattern = "\n") %>% # Tika
  unlist()

inspect(x = processing_text, m = 465, span = 12)
```
## Trim Whitespace
```{r}
processing_text <- stringr::str_trim(string = processing_text)
processing_text <- stringr::str_squish(string = processing_text)

inspect(x = processing_text, m = 465)
```

## Remove from References/Bibliography
```{r}
## Remove Anything From References / Bibliography to End
## Define Sections

### Return Logical Vector
logical_section <- ifelse(
  test = (
    tolower(processing_text) == "references" | 
    tolower(processing_text) == "bibliography"
    ),
  yes  = TRUE,
  no   = FALSE)


if (any(logical_section)){
  index <- min(which(logical_section == TRUE))
  processing_text <- processing_text[1:index-1]
}

processing_text[1610:1611]

length(processing_text)
```

## Remove Patterns
```{r}
# ## Remove Elements Which Match Removal Patterns
# processing_text <- processing_text[!processing_text %in% removal_patterns]
# 
# inspect()
```

## Drop Line W/ Only Numbers or Symbols
```{r}
## Drop lines with only numbers or symbols
processing_text <- remove_if_detect(
  input_vector   = processing_text,
  regex          = regex_letters,
  logical_method = "inverse"
)

inspect(x = processing_text, m = 220)

```

## Drop Vectors With Length 1 or Less
```{r}
## Drop elements with length of 1 or less
logical_length <- nchar(processing_text) > 1
processing_text <- processing_text[logical_length]

# Drop any NA elements
processing_text <- processing_text[!is.na(processing_text)]

inspect(x = processing_text, m = 220)
```

## Remove Months
```{r}
processing_text <- remove_if_detect(
  input_vector  = processing_text,
  remove_string = toupper(month.name),
  location      = "start"
)

## Drop Any NA Elements
processing_text <- processing_text[!is.na(processing_text)]

inspect(x = processing_text, m = 220)
```

## Concatenate Hyphens
```{r}
## Concatenate Adjacent Elements If Initial Element Ends With Hyphen
processing_text <- concat_hypen_vector(processing_text)

inspect(x = processing_text, m = 210)
```

## Downloaded Files
```{r}
## Remove elements which contain text related to downloading documents.

download_vec <- c('This content downloaded','http','jsto','DOI','doi')

processing_text <- remove_if_detect(
  input_vector  = processing_text,
  remove_string = download_vec,
  location      = "any"
)

inspect(x = processing_text, m = 210)
```

## IP Addresses
```{r}
## Remove elements which contain IP addresses

processing_text <- remove_if_detect(
  input_vector = processing_text,
  regex        = regex_ip,
  location     = "any"
)

inspect(x = processing_text, m = 210)
```

## Parenthesis
```{r}
# Parenthesis ----------------------------------------------------------------
## Remove text within parenthesis
### Define term to identify line splits
# line_split_indicator <- " -LINESPLIT-"
# 
# ### Concatenate all vector elements, separated by line split
# processing_text <- stringr::str_c(
#   processing_text,
#   collapse = line_split_indicator
# )
# 
# # Remove content within parenthesis
# processing_text <- stringr::str_remove_all(
#   string  = processing_text,
#   pattern = regex_parens
# )
# 
# # Split single string back into character vectors
# processing_text <- stringr::str_split(
#   string  = processing_text,
#   pattern = line_split_indicator) %>%
#   unlist()
```

## Empty Vectors
```{r}
# Empty Vectors --------------------------------------------------------------
## Drop empty vectors
processing_text <- processing_text[processing_text!=""]

## Drop NA elements
processing_text <- processing_text[!is.na(processing_text)]

inspect(x = processing_text, m = 210)
```

## Symbols and Numbers
```{r}
# Numbers and Symbols (Second Time) ------------------------------------------
## Drop lines with only numbers or symbols
processing_text <- remove_if_detect(
  input_vector   = processing_text,
  regex          = regex_letters,
  logical_method = "inverse"
)

inspect(x = processing_text, m = 210)
```

## Standardize Hypothesis / Proposition
```{r}
# Standardize Hypothesis/Propositions-----------------------------------------
## Hypothesis
processing_text <- standardize_hypothesis_proposition(
  input_string  = processing_text
)

## Drop object names
processing_text <- unname(processing_text)

inspect(x = processing_text, m = 210)
```
## Remove Trailing Period
```{r}
# Remove trailing period for standardizes hypothesis tags
processing_text <- remove_period(
  input_string = processing_text
)

## Drop object names
processing_text <- unname(processing_text)

inspect(x = processing_text, m = 210)
```


## Remove Periods From Common Abbreviations
```{r}
input_vector <- c(
   "managers in order to live in personally attractive regions and may be willing to substitute some",
   "managers in order to live in personally attractive regions et al.",
   "managers in order to live in personally attractive regions et al. i.e. e.g."  
)

# inspect(x = processing_text, m = 205)
  common_abbr <- c(
    " e.g.",
    " i.e.",
    " etc.",
    " et al.",
    " ibid.",
    " Ph.D.",
    " Q.E.D.",
    " q.e.d."
  )

  output_vector <- input_vector

  for (abbr in common_abbr) {
    abbr_wo_period <- stringr::str_remove_all(
      string = abbr,
      pattern = "\\."
      )

    print(abbr_wo_period)
    output_vector <- stringr::str_replace_all(
      string      = output_vector,
      pattern     = abbr,
      replacement = abbr_wo_period
    )
    # print(output_vector)
  }

  output_vector
```

```{r}

inspect(x = processing_text, m = 205)
processing_text <- remove_period_abbr(processing_text)

inspect(x = processing_text, m = 205)
```

## Tokenize Sentence - First Pass (Tokenizers)
```{r}
inspect(x = processing_text, m = 130, span = 10)

# Tokenize Sentences ---------------------------------------------------------
## Convert Vector Elements into Sentences
processing_text <- stringr::str_c(
  processing_text,
  collapse = " "
)

# processing_text

processing_text <- tokenizers::tokenize_sentences(
  processing_text,
  strip_punct = FALSE) %>%
  unlist()

# processing_text

## Replace double spaces with single
processing_text <- stringr::str_replace_all(
  string      = processing_text,
  pattern     = "  ",
  replacement = " "
)

inspect(x = processing_text, m = 90, span = 10)

```

## Tokenize Sentence - Second Pass (Stringr)
```{r}
processing_text <- stringr::str_split(
  string  = processing_text, 
  pattern = "\\.") %>% 
  unlist()

## Drop empty vectors
processing_text <- processing_text[processing_text!=""]

inspect(x = processing_text, m = 90, span = 20)
```
## Normalize Case 
```{r}
# Normalize Case -------------------------------------------------------------
## Set everything to lowercase
processing_text <- tolower(processing_text)
```

## Downloading (Second Time)
```{r}
# Downloading (Second Time) --------------------------------------------------
  ## Remove elements which contain terms related to downloading files
  processing_text <- remove_if_detect(
    input_vector  = processing_text,
    remove_string = download_vec,
    location      = "any"
  )

inspect(x = processing_text, m = 150, span = 10)
```

## Symbols and Numbers (Second Time)
```{r}
  # Numbers and Symbols (Third Time) -------------------------------------------
  ## Drop lines with only numbers or symbols
  processing_text <- remove_if_detect(
    input_vector   = processing_text,
    regex          = regex_letters,
    logical_method = "inverse"
  )

inspect(x = processing_text, m = 90 , span = 10)
```

## Split Sentences with Multiple Hypothesis Tags
```{r}
processing_text <- break_out_hypothesis_tags(processing_text)

inspect(x = processing_text, m = 90 , span = 20)
```

## Misc Text Replacement
```{r}
  # Misc Text Replacement ------------------------------------------------------
  ## Replace double colons
  processing_text <- stringr::str_replace_all(
    string      = processing_text,
    pattern     = ": :",
    replacement = ":"
  )

  ## Remove extra white space
  processing_text <- stringr::str_squish(
    string = processing_text
  )

  ## Replace colon/period instances (: .)
  processing_text <- stringr::str_replace_all(
    string      = processing_text,
    pattern     = ": \\.",
    replacement = ":"
  )
  

inspect(x = processing_text, m = 150, span = 10)
```

```{r}
length(processing_text)
processing_text[length(processing_text)]
```

# Final Function
```{r}
input_text <- process_text(input_path)

length(input_text)

processing_text[length(input_text)]
```


---
title: "Entity Extraction Performance Evaluation - R & Python"
author: "Evan Canfield"
date: "12/30/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Purpose
The purpose of this notebook is to test how to implement the Entity Extraction model 

# Import
## Libraries
```{r import_libraries}
  if (!require(pacman)) {install.packages('pacman')}
  p_load(
    dplyr
  )
```

## Source Files
The following imports functions defined in the sourced R scripts.
```{r}
# # Import All Scripts
# script_path <- "../R/"
# file_paths <- list.files(recursive = TRUE,
#                          path = script_path, pattern = ".R",
#                          full.names = TRUE)
#
# for (file in file_paths){
#   source(file)
# }
```

## Data
```{r}
folder_path <- "./../../../inst/extdata/sample_documents/"
pdf_paths <- list.files(recursive = FALSE, 
                       path = folder_path, 
                       pattern = ".pdf", 
                       full.names = TRUE)
print(pdf_paths)
```

## Assisting Functions
```{r}
#' Inspect Text String
inspect <- function(x, m = 100, span = 20) {
  n = m + span - 1
  x[m:n]
  
}
```

## Regex
```{r}
## Identify Letters
regex_letters <- '[a-zA-Z]'

## Identify IP Address
regex_ip <- "(?:[\\d]{1,3})\\.(?:[\\d]{1,3})\\.(?:[\\d]{1,3})\\.(?:[\\d]{1,3})"

## Identify Parenthesis
regex_parens <- "\\(([^()]+)\\)"

## Identify Hypothesis Formats
regex_hypo <- c(
  "h[0-9]{1,3}[a-zA-Z]\\:",
  "H[0-9]{1,3}[a-zA-Z]\\:",
  "h[0-9]{1,3}[a-zA-Z]\\.",
  "H[0-9]{1,3}[a-zA-Z]\\.",
  "h[0-9]{1,3}[a-zA-Z]",
  "H[0-9]{1,3}[a-zA-Z]",
  "hypothesis [0-9]{1,3}[a-zA-Z]\\:",
  "Hypothesis [0-9]{1,3}[a-zA-Z]\\:",
  "hypothesis [0-9]{1,3}[a-zA-Z]\\.",
  "Hypothesis [0-9]{1,3}[a-zA-Z]\\.",
  "hypothesis [0-9]{1,3}[a-zA-Z]",
  "Hypothesis [0-9]{1,3}[a-zA-Z]",
  "h[0-9]{1,3}\\:",
  "H[0-9]{1,3}\\:",
  "h[0-9]{1,3}\\.",
  "H[0-9]{1,3}\\.",
  "h[0-9]{1,3}",
  "H[0-9]{1,3}",
  "hypothesis [0-9]{1,3}\\:",
  "Hypothesis [0-9]{1,3}\\:",
  "hypothesis [0-9]{1,3}\\.",
  "Hypothesis [0-9]{1,3}\\.",
  "hypothesis [0-9]{1,3}",
  "Hypothesis [0-9]{1,3}"
  )

regex_proposition <- c(
  "p[0-9]{1,3}[a-zA-Z]\\:",
  "P[0-9]{1,3}[a-zA-Z]\\:",
  "p[0-9]{1,3}[a-zA-Z]\\.",
  "P[0-9]{1,3}[a-zA-Z]\\.",
  "p[0-9]{1,3}[a-zA-Z]",
  "P[0-9]{1,3}[a-zA-Z]",
  "proposition [0-9]{1,3}[a-zA-Z]\\:",
  "Proposition [0-9]{1,3}[a-zA-Z]\\:",
  "proposition [0-9]{1,3}[a-zA-Z]\\.",
  "Proposition [0-9]{1,3}[a-zA-Z]\\.",
  "proposition [0-9]{1,3}[a-zA-Z]",
  "Proposition [0-9]{1,3}[a-zA-Z]",
  "p[0-9]{1,3}\\:",
  "P[0-9]{1,3}\\:",
  "p[0-9]{1,3}\\.",
  "P[0-9]{1,3}\\.",
  "p[0-9]{1,3}",
  "P[0-9]{1,3}",
  "proposition [0-9]{1,3}\\:",
  "Proposition [0-9]{1,3}\\:",
  "proposition [0-9]{1,3}\\.",
  "Proposition [0-9]{1,3}\\.",
  "proposition [0-9]{1,3}",
  "Proposition [0-9]{1,3}"
)

## Identify Numbers
regex_return_num <- "(\\d)+"
```

# Process Data
```{r}
input_path <- pdf_paths[5]
input_path
input_text <- pdfminer$extract_text(input_path)
```

## Step 1
```{r}
processing_text <- input_text %>%
  stringr::str_split(pattern = "\r\n") %>% # Tabulizer
  stringr::str_split(pattern = "\n") %>% # Tika
  unlist()

processing_text[100:120]
```
## Step - Remove from References/Bibliography
```{r}
## Remove Anything From References / Bibliography to End
## Define Sections
section_key <- c("References", "Bibliography",
                 "REFERENCES", "BIBIOGRAPHY")

### Convert to regex
regex_section <- gen_regex(
  input_string = section_key,
  match        = "exact"
)

### Return Logical Vector
logical_section <- stringr::str_detect(
  string  = processing_text,
  pattern = regex_section)

if (any(logical_section)){
  index <- min(which(logical_section == TRUE))
  processing_text <- processing_text[1:index-1]
}
```

# Step - Remove Patterns
```{r}
# ## Remove Elements Which Match Removal Patterns
# processing_text <- processing_text[!processing_text %in% removal_patterns]
# 
# inspect()
```

## Step - Drop Line W/ Only Numbers or Symbols
```{r}
## Drop lines with only numbers or symbols
processing_text <- remove_if_detect(
  input_vector   = processing_text,
  regex          = regex_letters,
  logical_method = "inverse"
)

```

## Step - Drop Vectors With Length 1 or Less
```{r}
## Drop elements with length of 1 or less
logical_length <- nchar(processing_text) > 1
processing_text <- processing_text[logical_length]

# Drop any NA elements
processing_text <- processing_text[!is.na(processing_text)]
```

## Step - Remove Months
```{r}
processing_text <- remove_if_detect(
  input_vector  = processing_text,
  remove_string = toupper(month.name),
  location      = "start"
)

## Drop Any NA Elements
processing_text <- processing_text[!is.na(processing_text)]
```

## Step - Concatenate Hyphens
```{r}
## Concatenate Adjacent Elements If Initial Element Ends With Hyphen
processing_text <- concat_hypen_vector(processing_text)
```

## Downloaded Files
```{r}
## Remove elements which contain text related to downloading documents.

download_vec <- c('This content downloaded','http','jsto','DOI','doi')

processing_text <- remove_if_detect(
  input_vector  = processing_text,
  remove_string = download_vec,
  location      = "any"
)

```

## IP Addresses
```{r}
## Remove elements which contain IP addresses

processing_text <- remove_if_detect(
  input_vector = processing_text,
  regex        = regex_ip,
  location     = "any"
)

```

## Parenthesis
```{r}
# Parenthesis ----------------------------------------------------------------
## Remove text within parenthesis
### Define term to identify line splits
# line_split_indicator <- " -LINESPLIT-"
# 
# ### Concatenate all vector elements, separated by line split
# processing_text <- stringr::str_c(
#   processing_text,
#   collapse = line_split_indicator
# )
# 
# # Remove content within parenthesis
# processing_text <- stringr::str_remove_all(
#   string  = processing_text,
#   pattern = regex_parens
# )
# 
# # Split single string back into character vectors
# processing_text <- stringr::str_split(
#   string  = processing_text,
#   pattern = line_split_indicator) %>%
#   unlist()
```

## Empty Vectors
```{r}
# Empty Vectors --------------------------------------------------------------
## Drop empty vectors
processing_text <- processing_text[processing_text!=""]

## Drop NA elements
processing_text <- processing_text[!is.na(processing_text)]
```

## Symbols and Numbers
```{r}
# Numbers and Symbols (Second Time) ------------------------------------------
## Drop lines with only numbers or symbols
processing_text <- remove_if_detect(
  input_vector   = processing_text,
  regex          = regex_letters,
  logical_method = "inverse"
)

inspect(x = processing_text, m = 425, span = 10)
```
## Standardize Hypothesis
```{r}
## Generate regex identify hypotheses
regex_hypo_str <- gen_regex(
  input_string = regex_hypo,
  match        = "partial"
)

processing_text <- standardize_hypothesis(
  input_string            = processing_text,
  regex_hypothesis_string = regex_hypo_str
)

## Drop object names
processing_text <- unname(processing_text)

inspect(x = processing_text, m = 375, span = 10)
```

## Standardize Proposition
```{r}
## Generate regex identify hypotheses
regex_prop_str <- gen_regex(
  input_string = regex_proposition,
  match        = "partial"
)

processing_text <- standardize_hypothesis(
  input_string            = processing_text,
  regex_hypothesis_string = regex_prop_str
)

## Drop object names
processing_text <- unname(processing_text)

inspect(x = processing_text, m = 420, span = 20)
```

## Remove period next to hypothesis marker
```{r}
processing_text <- remove_period(
  input_string = processing_text
)

## Drop object names
processing_text <- unname(processing_text)

inspect(x = processing_text, m = 420, span = 20)
```


## Tokenize Sentences
```{r}
# Tokenize Sentences ---------------------------------------------------------
## Convert Vector Elements into Sentences
processing_text <- stringr::str_c(
  processing_text,
  collapse = " "
)

processing_text <- tokenizers::tokenize_sentences(
  processing_text,
  strip_punct = FALSE) %>%
  unlist()

## Replace double spaces with single
processing_text <- stringr::str_replace_all(
  string      = processing_text,
  pattern     = "  ",
  replacement = " "
)

inspect(x = processing_text, m = 130, span = 10)

```
## tokenize Sentence - Second Pass
```{r}
processing_text <- stringr::str_split(
  string  = processing_text, 
  pattern = "\\.") %>% 
  unlist()

## Drop empty vectors
processing_text <- processing_text[processing_text!=""]

inspect(x = processing_text, m = 150, span = 10)
```


## Downloading (Second Time)
```{r}
# Downloading (Second Time) --------------------------------------------------
  ## Remove elements which contain terms related to downloading files
  processing_text <- remove_if_detect(
    input_vector  = processing_text,
    remove_string = download_vec,
    location      = "any"
  )

inspect(x = processing_text, m = 150, span = 10)
```

## Symbols and Numbers (Second Time)
```{r}
  # Numbers and Symbols (Third Time) -------------------------------------------
  ## Drop lines with only numbers or symbols
  processing_text <- remove_if_detect(
    input_vector   = processing_text,
    regex          = regex_letters,
    logical_method = "inverse"
  )

inspect(x = processing_text, m = 90 , span = 10)
```

## Split Sentences with Multiple Hypothesis Tags
```{r}
processing_text <- break_out_hypothesis_tags(processing_text)

inspect(x = processing_text, m = 90 , span = 20)
```

## Misc Text Replacement
```{r}
  # Misc Text Replacement ------------------------------------------------------
  ## Replace double colons
  processing_text <- stringr::str_replace_all(
    string      = processing_text,
    pattern     = ": :",
    replacement = ":"
  )

  ## Remove extra white space
  processing_text <- stringr::str_squish(
    string = processing_text
  )

  ## Replace colon/period instances (: .)
  processing_text <- stringr::str_replace_all(
    string      = processing_text,
    pattern     = ": \\.",
    replacement = ":"
  )
  

inspect(x = processing_text, m = 150, span = 20)
```

# Component Function Testing
## Standardize Hypothesis
We need to adjust this function so that alpha numeric hypothesis numbers are grabbed.
### Inputs
```{r}
input_string_alphanum <- paste(
  "15, we would be unable to reject Hypothesis 1b"
)

input_string_no_hypo <- "This is a dummy"

input_string_num <- "This is Hypothesis 1 a purely numeric hypothesis"


## Generate regex identify hypotheses
regex_hypothesis_string <- gen_regex(
  input_string = regex_hypo,
  match        = "partial"
)
```

### Extract Hypothesis Phrases
```{r}
extract_phrase <- stringr::str_extract(
  string  = input_string_alphanum,
  pattern = regex_hypothesis_string
)

extract_phrase
```

### Extract Hypothesis Number
#### Existing
```{r}

extact_number <- stringr::str_extract(
  string  = extract_phrase,
  pattern = regex_return_num
)
extact_number
```

#### Update able to capture alpha numerics
```{r}
regex_return_alphanum <- '/\b(?![0-9]+\b)(?![a-z]+\b)[0-9a-z]+\b/i'

extact_number <- stringr::str_match(
  string  = extract_phrase,
  pattern = regex_return_alphanum
)
extact_number <- extract_phrase %>% stringr::str_split(pattern = " ")

extact_number <- extact_number[[1]][2]
```

```{r}
replacement_string <- paste0("<split>Hypo ", extact_number, ": ")
replacement_string
    # Replace hypothesis with new value
    output_string <- stringr::str_replace(
      string      = input_string_alphanum,
      pattern     = regex_hypothesis_string,
      replacement = replacement_string
    )
    
    output_string
```


```{r}
standardize_hypothesis <- Vectorize(
  function(input_string, regex_hypothesis_string){

    # Extract identified value
    extract_phrase <- stringr::str_extract(
      string  = input_string,
      pattern = regex_hypothesis_string
    )

    # Check if hypothesis detected
    if (!is.na(extract_phrase)){

      # Extract hypothesis number
      extact_number <- stringr::str_extract(
        string  = extract_phrase,
        pattern = regex_return_num
      )

      # Create new string
      replacement_string <- paste0("<split>Hypo ", extact_number, ": ")

      # Replace hypothesis with new value
      output_string <- stringr::str_replace(
        string      = input_string,
        pattern     = regex_hypothesis_string,
        replacement = replacement_string
      )

    } else {
      output_string <- input_string

    }

    output_string

  }
)
```

